% =============================================================================
% CUT-PASTE AUGMENTATION FOR ANOMALY SEGMENTATION - PAPER SECTION
% =============================================================================
% This file contains the technical description of the Cut-Paste implementation
% for training EoMT on anomaly segmentation. Both English and Italian versions.
% =============================================================================

\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=2.5cm}

\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=Python,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red}
}

\title{Cut-Paste Augmentation for Anomaly Segmentation:\\Implementation and Technical Details}
\author{AML Project - EoMT Extension}
\date{January 2026}

\begin{document}
\maketitle

% =============================================================================
% ENGLISH VERSION
% =============================================================================

\part*{English Version}

\section{Introduction and Motivation}

The fundamental challenge in anomaly segmentation is the absence of anomalous samples during training. Standard semantic segmentation models are trained exclusively on in-distribution (ID) classes, making them inherently limited in detecting out-of-distribution (OOD) objects at inference time. This section describes our implementation of the \textbf{Cut-Paste augmentation strategy}, which addresses this limitation by synthetically introducing anomalous objects during training.

\subsection{Why Cut-Paste Augmentation?}

Traditional anomaly detection approaches rely on uncertainty estimation methods such as Maximum Softmax Probability (MSP), Maximum Logit, or Entropy. These methods exploit the observation that neural networks tend to produce lower confidence predictions for unfamiliar inputs. However, they suffer from several limitations:

\begin{enumerate}
    \item \textbf{Indirect Detection}: The model never explicitly learns what constitutes an anomaly; it only learns to recognize known classes.
    \item \textbf{Overconfidence}: Deep neural networks are known to produce overconfident predictions even for OOD inputs, particularly when the input shares visual features with training data.
    \item \textbf{No Explicit Supervision}: Without anomaly examples during training, the model cannot learn discriminative features that distinguish anomalies from known classes.
\end{enumerate}

The Cut-Paste strategy addresses these limitations by:

\begin{itemize}
    \item \textbf{Explicit Anomaly Class}: Introducing a dedicated 20th class (in addition to the 19 Cityscapes classes) that the model learns to predict for anomalous regions.
    \item \textbf{Direct Supervision}: Providing pixel-level ground truth for anomalous objects, enabling the model to learn explicit anomaly features.
    \item \textbf{Diverse OOD Objects}: Using objects from COCO dataset that are semantically different from Cityscapes classes, ensuring the model learns to generalize to various anomaly types.
\end{itemize}

\subsection{Expected Performance Improvements}

By training with explicit anomaly supervision, we expect the following improvements:

\begin{enumerate}
    \item \textbf{Higher AUPRC}: The model can directly output high probability for the anomaly class, rather than relying on low confidence for known classes.
    \item \textbf{Lower FPR@95}: With explicit anomaly features learned, the model should produce fewer false positives while maintaining 95\% true positive rate.
    \item \textbf{Robustness}: The learned anomaly representation should generalize better to unseen anomaly types compared to uncertainty-based methods.
\end{enumerate}


\section{Implementation Details}

Our Cut-Paste implementation consists of five main components, executed in the following order during the data preparation and training pipeline:

\subsection{Step 1: COCO OOD Subset Download (\texttt{download\_coco.py})}

The first step in our pipeline is the automated download and preprocessing of Out-of-Distribution (OOD) objects from the Microsoft COCO dataset. The COCO dataset, originally designed for object detection and instance segmentation, contains over 80 object categories with high-quality instance-level annotations, making it an ideal source for extracting diverse anomaly objects.

The \texttt{download\_coco.py} script connects to the COCO API to programmatically fetch images and their corresponding segmentation masks. For each image in the COCO training set, the script extracts individual object instances using their polygon annotations, creating cropped RGB images paired with binary masks that precisely delineate object boundaries.

The key insight motivating the category filtering is that certain COCO categories semantically overlap with Cityscapes classes. If we were to paste a COCO ``car'' onto a Cityscapes image and label it as an anomaly, we would be teaching the model that cars are anomalies---directly contradicting the Cityscapes labels where cars are a valid class. This would introduce severe label noise and degrade both segmentation and anomaly detection performance.

\subsubsection{Category Filtering}

We systematically exclude 7 categories that have direct semantic correspondence with Cityscapes classes:
\begin{itemize}
    \item \texttt{person} (COCO ID: 1) $\rightarrow$ overlaps with Cityscapes ``person''
    \item \texttt{car} (COCO ID: 3) $\rightarrow$ overlaps with Cityscapes ``car''
    \item \texttt{motorcycle} (COCO ID: 4) $\rightarrow$ overlaps with Cityscapes ``motorcycle''
    \item \texttt{bus} (COCO ID: 6) $\rightarrow$ overlaps with Cityscapes ``bus''
    \item \texttt{train} (COCO ID: 7) $\rightarrow$ overlaps with Cityscapes ``train''
    \item \texttt{truck} (COCO ID: 8) $\rightarrow$ overlaps with Cityscapes ``truck''
    \item \texttt{bicycle} (COCO ID: 2) $\rightarrow$ overlaps with Cityscapes ``bicycle''
\end{itemize}

The remaining 73 COCO categories span a diverse range of object types that would never appear in urban driving scenarios: animals (elephant, giraffe, dog, cat, bear), furniture (chair, couch, bed, dining table), electronics (laptop, keyboard, cell phone, remote), food items (pizza, cake, banana, apple), sports equipment (baseball bat, tennis racket, skateboard), and many others. This diversity is crucial for training the model to recognize anomalies of varying shapes, sizes, textures, and semantic content.

\subsubsection{Output Structure}

The download script produces a carefully organized directory structure optimized for the subsequent Cut-Paste generation phase:
\begin{lstlisting}
coco_ood_subset/
    images/          # Cropped RGB images of individual OOD objects
    masks/           # Binary instance masks (255=object, 0=background)
    metadata.json    # JSON with object info: bbox, category_id, 
                     # category_name, original_image_id, area
\end{lstlisting}

The \texttt{images/} directory contains tightly cropped RGB patches centered on each object instance, extracted using the bounding box annotations. The \texttt{masks/} directory stores corresponding binary masks where pixel value 255 indicates the object region and 0 indicates background. These masks are derived from COCO's polygon annotations, rasterized to match the crop dimensions. The \texttt{metadata.json} file maintains a comprehensive record of each object's properties, enabling downstream filtering by size, category, or other criteria if needed.

\subsection{Step 2: Cut-Paste Dataset Generation (\texttt{generate\_cutpaste\_dataset.py})}

The \texttt{generate\_cutpaste\_dataset.py} script is the core of our augmentation pipeline. It takes the original Cityscapes training set and the downloaded COCO OOD objects, and produces a new dataset where anomalous objects have been synthetically inserted into the urban driving scenes.

We chose an \textbf{offline augmentation} strategy rather than applying Cut-Paste on-the-fly during training. This design decision was driven by several practical considerations:
\begin{itemize}
    \item \textbf{Training Speed}: Real-time augmentation would add computational overhead to each training iteration. Loading pre-augmented images is as fast as loading original images, avoiding any slowdown in the training loop.
    \item \textbf{Reproducibility}: By generating the augmented dataset once, we guarantee that every training run sees exactly the same augmented images. This eliminates a source of randomness and makes experiments more reproducible.
    \item \textbf{Visual Inspection}: Before committing to a full training run (which may take hours or days), we can visually inspect the generated images to verify that the augmentation looks realistic. This allows catching errors early.
    \item \textbf{Debugging}: If the model fails to detect anomalies, we can examine the training images to determine whether the issue lies in the augmentation process or the model itself.
\end{itemize}

The script is implemented as a Python class \texttt{CutPasteGenerator} that encapsulates all augmentation logic. It uses multiprocessing to parallelize the generation across CPU cores, significantly reducing the total generation time for the 2,975 Cityscapes training images.

\subsubsection{Augmentation Pipeline}

The augmentation pipeline processes each of the 2,975 Cityscapes training images according to the following procedure:

\textbf{Step A: Augmentation Decision.} For each image, a coin flip with probability $p = 0.5$ determines whether Cut-Paste augmentation will be applied. This means approximately half of the training images will contain synthetic anomalies, while the other half remain unchanged. This balance is important: if all images contained anomalies, the model might learn to always predict anomalies; conversely, too few anomaly images would provide insufficient supervision for the anomaly class.

\textbf{Step B: Object Selection.} If augmentation is applied, we randomly select $n \sim \mathcal{U}(1, 3)$ objects from the COCO OOD pool. The uniform distribution ensures variety in scene complexity---some images will have a single anomaly, others will have up to three. The objects are selected completely at random from all available categories, promoting diversity in anomaly appearance.

\textbf{Step C: Object Processing.} For each selected object, we perform the following transformations:
\begin{enumerate}
    \item \textbf{Loading}: Read the object's RGB crop and binary mask from disk.
    \item \textbf{Scaling}: Apply a random scale factor $s \sim \mathcal{U}(0.5, 1.5)$ to both the crop and mask. To prevent unrealistically large objects, we cap the final size at 40\% of the target image dimensions. Objects that would exceed this limit are scaled down proportionally.
    \item \textbf{Positioning}: Select a random position $(x, y)$ on the target image. The position is chosen uniformly at random, allowing objects to appear anywhere including near image borders. Objects extending beyond image boundaries are simply clipped.
    \item \textbf{Blending}: Apply alpha compositing using the feathered mask (detailed below) to seamlessly merge the object with the background.
    \item \textbf{Label Update}: In the segmentation ground truth, all pixels covered by the object (where mask $> 0.5$) are assigned the special anomaly label ID = 254.
\end{enumerate}

\subsubsection{Alpha Feathering Technique}

A naive Cut-Paste implementation would simply overwrite background pixels with object pixels wherever the binary mask equals 1. However, this creates unnaturally sharp edges at object boundaries---edges that would never occur in real photographs. A model trained on such data might learn to detect anomalies by looking for these artificial edge artifacts rather than learning genuine anomaly features. This would lead to poor generalization on real anomaly benchmarks where objects have natural edges.

To address this problem, we implement an \textbf{alpha feathering technique} that creates soft, gradual transitions between pasted objects and the background. The technique works by converting the binary mask into a continuous alpha channel with values smoothly transitioning from 1 (fully opaque) in the object interior to 0 (fully transparent) at the edges.

\begin{algorithm}
\caption{Feathered Mask Creation}
\begin{algorithmic}[1]
\State $M_{float} \gets M_{binary} / 255.0$ \Comment{Normalize to [0, 1]}
\State $M_{eroded} \gets \text{Erode}(M_{float}, \text{kernel}=3\times3, \text{iterations}=2)$
\State $M_{blurred} \gets \text{GaussianBlur}(M_{float}, \sigma=5)$
\State $M_{feathered} \gets \max(M_{eroded}, M_{blurred})$
\State \Return $M_{feathered}$
\end{algorithmic}
\end{algorithm}

The algorithm combines two complementary operations:
\begin{itemize}
    \item \textbf{Erosion} ($3\times3$ kernel, 2 iterations): Morphological erosion shrinks the mask inward, removing approximately 6 pixels from the boundary. This creates a \textbf{solid core} region where $\alpha = 1$, ensuring that the central part of the object is rendered at full opacity. Without erosion, excessive blurring could make small objects appear ghost-like.
    \item \textbf{Gaussian Blur} ($\sigma = 5$): Blurring the original binary mask creates a smooth gradient from 1 inside the object to 0 outside. The gradient width is controlled by $\sigma$; larger values create wider, softer transitions.
\end{itemize}

The $\max$ operation combines these two masks: the eroded mask guarantees a solid center, while the blurred mask provides \textbf{soft edges}. The result is a mask that transitions smoothly from opaque interior to transparent exterior over a band of approximately 10-15 pixels.

The final blending follows the standard Porter-Duff alpha compositing formula:
\begin{equation}
    I_{output}(x,y) = \alpha(x,y) \cdot I_{object}(x,y) + (1 - \alpha(x,y)) \cdot I_{background}(x,y)
\end{equation}

This pixel-wise interpolation ensures that edge pixels contain a weighted mixture of object and background colors, simulating the natural anti-aliasing that occurs in real photographs.

\subsubsection{Output Structure}

The script produces a Cityscapes-compatible directory structure:
\begin{lstlisting}
cityscapes_cutpaste/
    leftImg8bit/
        train/
            {city}/
                {city}_XXXXXX_YYYYYY_leftImg8bit.png
    gtFine/
        train/
            {city}/
                {city}_XXXXXX_YYYYYY_gtFine_labelIds.png
    metadata_train.json   # Statistics and file list
\end{lstlisting}

\subsection{Step 3: Dataset Packaging (\texttt{zip\_dataset.py})}

The generated Cut-Paste dataset can be quite large: each Cityscapes image is 2048$\times$1024 pixels, and with 2,975 training images plus their corresponding label maps, the uncompressed dataset exceeds 10 GB. Transferring such large datasets between machines---for example, from a local workstation where augmentation was performed to a cloud GPU cluster for training---can be time-consuming and costly.

The \texttt{zip\_dataset.py} script addresses this practical concern by compressing the entire generated dataset into a single ZIP archive using the \texttt{ZIP\_DEFLATED} compression algorithm. This standard compression method achieves good compression ratios on PNG images while maintaining broad compatibility across operating systems.

The script implements several features for robustness:
\begin{itemize}
    \item \textbf{Progress Tracking}: A progress bar shows compression progress for large datasets
    \item \textbf{Directory Structure Preservation}: The ZIP archive maintains the exact Cityscapes-compatible directory hierarchy, so extraction produces a ready-to-use dataset
    \item \textbf{Selective Compression}: Only necessary files (images and labels) are included; temporary files and caches are excluded
\end{itemize}

The resulting ZIP file can be uploaded to cloud storage, transferred via network, or archived for reproducibility purposes.

\subsection{Step 4: Training Configuration (\texttt{eomt\_base\_640\_cutpaste.yaml})}

The EoMT framework uses YAML configuration files to specify all training hyperparameters, model architecture, and data pipeline settings. For Cut-Paste training, we created a new configuration file \texttt{eomt\_base\_640\_cutpaste.yaml} that extends the standard EoMT semantic segmentation setup with the modifications necessary for anomaly-aware training.

The most significant changes are:

\begin{lstlisting}[language=yaml]
model:
  init_args:
    num_classes: 20  # 19 Cityscapes + 1 anomaly
    load_ckpt_class_head: false  # Reinitialize classifier

data:
  class_path: datasets.cityscapes_semantic_cutpaste.CityscapesSemanticCutPaste
  init_args:
    path: "/path/to/cityscapes_cutpaste"
    original_cityscapes_path: "/path/to/original/cityscapes"
\end{lstlisting}

\textbf{Number of Classes}: The standard Cityscapes configuration uses \texttt{num\_classes: 19}, corresponding to the 19 evaluation classes in the Cityscapes benchmark. We increase this to \texttt{num\_classes: 20} to add the anomaly class as the 20th output channel. This change propagates through the entire model architecture: the final classification head now outputs 20 logits per pixel instead of 19.

\textbf{Classification Head Reinitialization}: The \texttt{load\_ckpt\_class\_head: false} parameter is critical for correct training. We start from a pretrained EoMT checkpoint that was trained on standard 19-class Cityscapes. The classification head in this checkpoint has shape \texttt{[hidden\_dim, 19]}. If we attempted to load these weights into a 20-class model (shape \texttt{[hidden\_dim, 20]}), we would get a dimension mismatch error. By setting this parameter to \texttt{false}, we instruct the training script to skip loading the classification head weights and instead randomly initialize the new 20-class head. The rest of the model (backbone, transformer layers, positional embeddings) is still loaded from the pretrained checkpoint, preserving the learned visual representations.

\textbf{Custom Data Module}: We specify our custom \texttt{CityscapesSemanticCutPaste} data module, which knows how to load the augmented dataset and parse the anomaly labels correctly. The configuration provides two paths: the path to the Cut-Paste augmented dataset (used for training) and the path to the original Cityscapes (used for validation).

\subsection{Step 5: Custom DataModule (\texttt{cityscapes\_semantic\_cutpaste.py})}

The final piece of the pipeline is the PyTorch Lightning DataModule that integrates the Cut-Paste augmented dataset with the EoMT training framework. This module extends the standard Cityscapes semantic segmentation DataModule with custom logic for handling the anomaly class.

The DataModule is responsible for:
\begin{itemize}
    \item Loading images and label maps from the correct directories
    \item Applying training-time augmentations (random crops, flips, color jitter)
    \item Parsing label maps into the format expected by the EoMT loss function
    \item Creating DataLoader objects with appropriate batch sizes and workers
\end{itemize}

\subsubsection{Target Parsing}

The most significant customization is the \texttt{target\_parser} method, which converts raw label maps into the mask-based format used by EoMT's mask classification loss. The key challenge is correctly handling the anomaly label ID = 254, which does not exist in the standard Cityscapes label scheme.

\begin{lstlisting}[language=Python]
def target_parser(target, anomaly_label_id=254, **kwargs):
    masks, labels = [], []
    for label_id in target[0].unique():
        if label_id == anomaly_label_id:
            # Special handling for anomaly pixels
            masks.append(target[0] == label_id)
            labels.append(19)  # Anomaly class = train_id 19
            continue
        # Standard Cityscapes class handling:
        # Convert Cityscapes label_id to train_id...
    return masks, labels, is_crowd
\end{lstlisting}

The parsing logic iterates over all unique label IDs present in the ground truth. When it encounters the special value 254, it creates a binary mask for those pixels and assigns them to class 19 (the anomaly class, which is the 20th class with 0-based indexing). All other label IDs are processed using the standard Cityscapes label-to-trainId mapping.

This design choice---using label ID 254 in the raw files and mapping it to train\_id 19 at runtime---maintains compatibility with the Cityscapes file format while adding anomaly support.

\subsubsection{Dataset Structure}

The DataModule implements a crucial asymmetry between training and validation:
\begin{itemize}
    \item \textbf{Training Set}: Uses the pre-generated Cut-Paste augmented images from \texttt{cityscapes\_cutpaste/}. These images may contain pasted OOD objects labeled with ID 254.
    \item \textbf{Validation Set}: Uses the original, unmodified Cityscapes validation images from the standard dataset location. These images contain only the 19 standard classes with no anomalies.
\end{itemize}

This separation serves two purposes:
\begin{enumerate}
    \item \textbf{Fair Evaluation}: Validation mIoU is computed on clean Cityscapes data, allowing direct comparison with baseline models. If validation also contained synthetic anomalies, the mIoU metric would be artificially affected.
    \item \textbf{Generalization Testing}: The model learns to detect anomalies from COCO objects but is evaluated on real anomaly benchmarks (RoadAnomaly, etc.) with completely different anomaly types. This tests true generalization rather than memorization.
\end{enumerate}


\section{Model Modifications}

While the core EoMT architecture remains unchanged, several modifications to the training infrastructure were necessary to support Cut-Paste training. These changes are implemented in the Lightning module (\texttt{lightning\_module.py}) and handle compatibility issues when loading pretrained checkpoints.

\subsection{Lightning Module Changes (\texttt{lightning\_module.py})}

The PyTorch Lightning module orchestrates the training loop, checkpoint loading, and logging. Two key modifications were made to handle the specific requirements of our Cut-Paste training setup:

\subsubsection{Positional Embedding Interpolation}

Vision Transformers (ViT), including the DINOv2 backbone used in EoMT, use learned positional embeddings to encode spatial information. These embeddings are trained at a specific resolution and have a fixed spatial dimension. For example, a model trained at 1024$\times$1024 resolution with a patch size of 16 has positional embeddings of shape $(64 \times 64) = 4096$ tokens.

When we want to train at a different resolution (e.g., 640$\times$640, which gives $(40 \times 40) = 1600$ tokens), we cannot directly use the pretrained positional embeddings because the dimensions don't match. Simply truncating or zero-padding would destroy the learned spatial relationships.

Our solution is \textbf{bicubic interpolation} of the positional embeddings:

\begin{equation}
    \text{pos\_embed}_{new} = \text{Interpolate}_{bicubic}\left(\text{pos\_embed}_{ckpt}, \left(\frac{H_{new}}{16}, \frac{W_{new}}{16}\right)\right)
\end{equation}

The interpolation process:
\begin{enumerate}
    \item Reshape the 1D positional embeddings back to 2D spatial grid (excluding the CLS token if present)
    \item Apply bicubic interpolation to resize the grid to the new spatial dimensions
    \item Flatten back to 1D and concatenate with the (unchanged) CLS token embedding
\end{enumerate}

Bicubic interpolation is preferred over bilinear because it produces smoother results with less aliasing. This technique preserves the relative spatial encoding learned during pretraining while adapting to the new resolution.

\subsubsection{Parameter Purification}

Certain deep learning operations, particularly those involving Fourier transforms (FFT), can produce complex-valued tensors with real and imaginary components. While these are mathematically valid during forward passes, they can cause issues during checkpoint saving/loading and gradient computation.

We encountered checkpoints where some parameters had inadvertently become complex-valued (\texttt{torch.complex64}). To ensure training stability and compatibility, we implement a \textbf{parameter purification} step that forces all parameters to real-valued float32:

\begin{lstlisting}[language=Python]
def _purify_parameters(module):
    """Convert any complex parameters to real float32."""
    for name, param in module.named_parameters():
        if torch.is_complex(param):
            # Extract real part and create new parameter
            new_data = param.data.real.detach().clone().float()
            # Replace the parameter in the module
            setattr(module, param_name, nn.Parameter(new_data))
\end{lstlisting}

This function traverses all parameters in the model. For any parameter with complex dtype, it extracts the real component, converts to float32, and replaces the original parameter. This ensures the entire model uses consistent real-valued parameters suitable for standard gradient descent optimization.


\section{Evaluation Metrics}

To evaluate our Cut-Paste trained model, we developed a custom evaluation script (\texttt{evalAnomaly\_cut\_paste.py}) that computes both traditional anomaly detection metrics and our new learned anomaly metric. The script processes standard anomaly segmentation benchmarks and produces comprehensive performance reports.

\subsection{Standard Metrics (Normalized on 19 Classes)}

For fair comparison with baseline methods that were trained on standard 19-class Cityscapes, we compute traditional uncertainty-based anomaly scores using only the first 19 output logits. This ensures we're comparing apples to apples: the baseline methods only see 19 classes, so we normalize our model's predictions to the same 19-class space when computing these metrics.

Given an input image, let $f_c(x,y)$ denote the logit (pre-softmax activation) for class $c$ at pixel location $(x,y)$, and let $P(c|x,y) = \text{softmax}(f)_c$ denote the corresponding probability. The standard metrics are:

\begin{itemize}
    \item \textbf{MSP (Maximum Softmax Probability)}: 
    \begin{equation}
        \text{MSP}(x,y) = 1 - \max_{c \in \{1,...,19\}} P(c|x,y)
    \end{equation}
    The intuition is that in-distribution pixels should have high confidence for one of the known classes, while OOD pixels should have lower maximum probability. We use $1 - \max P$ so that higher scores indicate higher anomaly likelihood.
    
    \item \textbf{MaxLogit}: 
    \begin{equation}
        \text{MaxLogit}(x,y) = -\max_{c \in \{1,...,19\}} f_c(x,y)
    \end{equation}
    Similar to MSP but operates on raw logits rather than probabilities. The negation ensures higher scores indicate anomalies. MaxLogit is often preferred over MSP because softmax can saturate and hide useful information in the logits.
    
    \item \textbf{Entropy}: 
    \begin{equation}
        \text{Entropy}(x,y) = -\sum_{c=1}^{19} P(c|x,y) \log P(c|x,y)
    \end{equation}
    Measures the uncertainty in the probability distribution. In-distribution pixels should have peaky distributions (low entropy) while OOD pixels should have more uniform distributions (high entropy).
    
    \item \textbf{RbA (Residual-based Anomaly)}: 
    \begin{equation}
        \text{RbA}(x,y) = 1 - \sum_{c=1}^{19} P(c|x,y)
    \end{equation}
    This metric exploits the fact that softmax probabilities must sum to 1. When normalized over only 19 classes (ignoring class 20), the sum may be less than 1 if the model assigns probability to the 20th class. However, for standard 19-class models, this equals zero everywhere.
\end{itemize}

\subsection{Learned Anomaly Metric}

The key advantage of our Cut-Paste training is enabling a new metric that directly uses the learned anomaly class:
\begin{equation}
    \text{Learned\_Anomaly}(x,y) = P(\text{class}=20 | x, y) = \text{softmax}(f)_{20}
\end{equation}

Unlike the uncertainty-based metrics above, which are \emph{indirect} measures of anomaly likelihood (based on the model being uncertain or having low confidence), the Learned\_Anomaly score is a \emph{direct} prediction. The model explicitly outputs how likely each pixel is to be an anomaly, trained with direct supervision from the Cut-Paste augmented ground truth.

To compute this metric, we apply softmax over all 20 logits (not just the first 19) and extract the probability assigned to class 20 (index 19 in 0-based indexing). This probability directly answers the question: ``Does the model think this pixel is an anomaly?''

\subsection{Benchmark Metrics}

For each anomaly score (MSP, MaxLogit, Entropy, RbA, Learned\_Anomaly), we compute two standard evaluation metrics by comparing against ground truth anomaly masks:

\begin{itemize}
    \item \textbf{AUPRC} (Area Under Precision-Recall Curve): The precision-recall curve plots precision against recall at various threshold settings. AUPRC summarizes this curve as a single number between 0 and 1. This metric is particularly appropriate for anomaly detection because:
    \begin{itemize}
        \item Anomalies are typically rare (class imbalance), and AUPRC is more informative than ROC-AUC in imbalanced settings
        \item A random classifier achieves AUPRC equal to the positive class proportion (often very low), making improvements more meaningful
        \item Higher AUPRC indicates better overall detection quality
    \end{itemize}
    
    \item \textbf{FPR@95} (False Positive Rate at 95\% True Positive Rate): This metric answers a practical question: ``If we want to detect 95\% of all anomalies, how many false alarms will we have?'' 
    \begin{itemize}
        \item We find the threshold that achieves 95\% true positive rate (detecting 95\% of actual anomaly pixels)
        \item At this threshold, we measure what fraction of normal pixels are incorrectly flagged as anomalies
        \item Lower FPR@95 indicates fewer false alarms while maintaining high detection rate
    \end{itemize}
\end{itemize}


\section{Experimental Results}

We evaluate our Cut-Paste trained EoMT model on five standard anomaly segmentation benchmarks. These benchmarks contain real-world images with genuine anomalous objects (not synthetically pasted), providing a true test of generalization from synthetic training data to real anomalies.

\subsection{Evaluation Benchmarks}

\begin{itemize}
    \item \textbf{RoadAnomaly}: A benchmark focusing on diverse road obstacles and unusual objects encountered during driving. Contains 60 images with pixel-level anomaly annotations.
    
    \item \textbf{RoadAnomaly21}: An updated version of RoadAnomaly with additional images and improved annotations. Part of the SegmentMeIfYouCan benchmark suite.
    
    \item \textbf{RoadObsticle21}: Focuses specifically on obstacles on the road surface (debris, lost cargo, animals) that a vehicle must detect and avoid. Also part of SegmentMeIfYouCan.
    
    \item \textbf{FS\_LostFound\_full}: The Fishyscapes Lost\&Found benchmark, containing images from the Lost\&Found dataset with small obstacles on roads.
    
    \item \textbf{fs\_static}: Fishyscapes Static benchmark where anomalous objects are synthetically composited into Cityscapes validation images. While synthetic, it provides a controlled evaluation setting.
\end{itemize}

Table \ref{tab:results} shows the performance comparison across these benchmarks.

\begin{table}[h]
\centering
\caption{Anomaly Segmentation Results (AUPRC \% / FPR@95 \%)}
\label{tab:results}
\begin{tabular}{l|cc|cc|cc}
\toprule
\textbf{Dataset} & \multicolumn{2}{c|}{\textbf{MaxLogit}} & \multicolumn{2}{c|}{\textbf{Entropy}} & \multicolumn{2}{c}{\textbf{Learned\_Anomaly}} \\
& AUPRC & FPR & AUPRC & FPR & AUPRC & FPR \\
\midrule
RoadAnomaly & 15.54 & 71.19 & 21.72 & 91.34 & \textbf{25.11} & 91.34 \\
RoadAnomaly21 & 22.49 & 76.36 & 21.06 & 95.42 & \textbf{25.17} & 95.42 \\
RoadObsticle21 & 10.75 & 56.73 & 6.90 & 75.42 & \textbf{25.56} & 57.74 \\
FS\_LostFound & 0.37 & 74.33 & 0.34 & 96.40 & \textbf{0.47} & 96.40 \\
fs\_static & 2.23 & 84.11 & 2.19 & 95.26 & \textbf{2.88} & 95.26 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

The Learned\_Anomaly metric consistently outperforms traditional uncertainty-based methods in terms of AUPRC across all benchmarks:

\begin{itemize}
    \item \textbf{RoadObsticle21}: The most dramatic improvement is observed here, with AUPRC increasing from 10.75\% (MaxLogit) to 25.56\% (+137\% relative improvement). This benchmark contains obstacles that are clearly out-of-distribution (animals, debris), which the model has learned to explicitly recognize through Cut-Paste training.
    
    \item \textbf{RoadAnomaly}: AUPRC improves from 21.72\% (Entropy) to 25.11\% (+16\% relative improvement). The improvement is more modest here, possibly because the anomalies in this benchmark are more diverse and challenging.
    
    \item \textbf{FS\_LostFound and fs\_static}: While absolute numbers are lower (these benchmarks contain small, challenging obstacles), the Learned\_Anomaly metric still achieves the best AUPRC among all methods.
\end{itemize}

\subsection{Discussion}

These results demonstrate several key findings:

\begin{enumerate}
    \item \textbf{Explicit supervision helps}: Despite being trained on synthetic COCO objects pasted onto Cityscapes, the model successfully generalizes to detect real anomalies of completely different types. This validates our Cut-Paste strategy.
    
    \item \textbf{Domain gap exists}: The improvements, while significant, are not uniform across benchmarks. Some anomaly types may be more similar to COCO training objects than others, leading to varying performance.
    
    \item \textbf{FPR@95 trade-offs}: The Learned\_Anomaly metric shows mixed results on FPR@95. In some cases (RoadObsticle21), it maintains competitive FPR while dramatically improving AUPRC. In other cases, FPR increases. This suggests potential for threshold tuning based on application requirements.
    
    \item \textbf{Baseline preservation}: Importantly, our modifications do not degrade standard segmentation performance. The model still achieves comparable mIoU on Cityscapes validation to the baseline (not shown in this table), demonstrating that learning to detect anomalies does not come at the cost of recognizing known classes.
\end{enumerate}


% =============================================================================
% ITALIAN VERSION
% =============================================================================

\newpage
\part*{Versione Italiana}

\section{Introduzione e Motivazione}

La sfida fondamentale nella segmentazione delle anomalie è l'assenza di campioni anomali durante il training. I modelli standard di segmentazione semantica vengono addestrati esclusivamente su classi in-distribution (ID), rendendoli intrinsecamente limitati nel rilevare oggetti out-of-distribution (OOD) durante l'inferenza. Questa sezione descrive la nostra implementazione della \textbf{strategia di augmentation Cut-Paste}, che affronta questa limitazione introducendo sinteticamente oggetti anomali durante il training.

\subsection{Perché Cut-Paste Augmentation?}

Gli approcci tradizionali per il rilevamento delle anomalie si basano su metodi di stima dell'incertezza come Maximum Softmax Probability (MSP), Maximum Logit o Entropy. Questi metodi sfruttano l'osservazione che le reti neurali tendono a produrre predizioni con confidenza inferiore per input non familiari. Tuttavia, soffrono di diverse limitazioni:

\begin{enumerate}
    \item \textbf{Rilevamento Indiretto}: Il modello non impara mai esplicitamente cosa costituisce un'anomalia; impara solo a riconoscere le classi note.
    \item \textbf{Overconfidence}: Le reti neurali profonde sono note per produrre predizioni eccessivamente sicure anche per input OOD, particolarmente quando l'input condivide caratteristiche visive con i dati di training.
    \item \textbf{Nessuna Supervisione Esplicita}: Senza esempi di anomalie durante il training, il modello non può imparare caratteristiche discriminative che distinguano le anomalie dalle classi note.
\end{enumerate}

La strategia Cut-Paste affronta queste limitazioni tramite:

\begin{itemize}
    \item \textbf{Classe Anomalia Esplicita}: Introducendo una 20ª classe dedicata (in aggiunta alle 19 classi Cityscapes) che il modello impara a predire per le regioni anomale.
    \item \textbf{Supervisione Diretta}: Fornendo ground truth a livello di pixel per gli oggetti anomali, permettendo al modello di imparare caratteristiche esplicite delle anomalie.
    \item \textbf{Oggetti OOD Diversificati}: Utilizzando oggetti dal dataset COCO semanticamente diversi dalle classi Cityscapes, assicurando che il modello impari a generalizzare a vari tipi di anomalie.
\end{itemize}

\subsection{Miglioramenti Prestazionali Attesi}

Addestrando con supervisione esplicita delle anomalie, ci aspettiamo i seguenti miglioramenti:

\begin{enumerate}
    \item \textbf{AUPRC più alto}: Il modello può direttamente produrre alta probabilità per la classe anomalia, invece di affidarsi a bassa confidenza per le classi note.
    \item \textbf{FPR@95 più basso}: Con caratteristiche esplicite delle anomalie apprese, il modello dovrebbe produrre meno falsi positivi mantenendo il 95\% di true positive rate.
    \item \textbf{Robustezza}: La rappresentazione appresa delle anomalie dovrebbe generalizzare meglio a tipi di anomalie non visti rispetto ai metodi basati sull'incertezza.
\end{enumerate}


\section{Dettagli Implementativi}

La nostra implementazione Cut-Paste consiste in cinque componenti principali, eseguiti nel seguente ordine durante la pipeline di preparazione dati e training:

\subsection{Step 1: Download Subset COCO OOD (\texttt{download\_coco.py})}

Il primo passo della nostra pipeline è il download automatizzato e il preprocessing degli oggetti Out-of-Distribution (OOD) dal dataset Microsoft COCO. Il dataset COCO, originariamente progettato per object detection e instance segmentation, contiene oltre 80 categorie di oggetti con annotazioni di alta qualità a livello di istanza, rendendolo una fonte ideale per estrarre oggetti anomali diversificati.

Lo script \texttt{download\_coco.py} si connette all'API COCO per recuperare programmaticamente le immagini e le loro maschere di segmentazione corrispondenti. Per ogni immagine nel training set COCO, lo script estrae le singole istanze di oggetti utilizzando le annotazioni poligonali, creando immagini RGB ritagliate accoppiate con maschere binarie che delineano precisamente i confini degli oggetti.

L'intuizione chiave che motiva il filtraggio delle categorie è che alcune categorie COCO si sovrappongono semanticamente con le classi Cityscapes. Se incollassimo una ``auto'' COCO su un'immagine Cityscapes e la etichettassimo come anomalia, staremmo insegnando al modello che le auto sono anomalie---contraddicendo direttamente le etichette Cityscapes dove le auto sono una classe valida. Questo introdurrebbe un grave rumore nelle etichette e degraderebbe sia le performance di segmentazione che di rilevamento anomalie.

\subsubsection{Filtraggio Categorie}

Escludiamo sistematicamente 7 categorie che hanno corrispondenza semantica diretta con le classi Cityscapes:
\begin{itemize}
    \item \texttt{person} (COCO ID: 1) $\rightarrow$ si sovrappone a Cityscapes ``person''
    \item \texttt{car} (COCO ID: 3) $\rightarrow$ si sovrappone a Cityscapes ``car''
    \item \texttt{motorcycle} (COCO ID: 4) $\rightarrow$ si sovrappone a Cityscapes ``motorcycle''
    \item \texttt{bus} (COCO ID: 6) $\rightarrow$ si sovrappone a Cityscapes ``bus''
    \item \texttt{train} (COCO ID: 7) $\rightarrow$ si sovrappone a Cityscapes ``train''
    \item \texttt{truck} (COCO ID: 8) $\rightarrow$ si sovrappone a Cityscapes ``truck''
    \item \texttt{bicycle} (COCO ID: 2) $\rightarrow$ si sovrappone a Cityscapes ``bicycle''
\end{itemize}

Le rimanenti 73 categorie COCO comprendono una gamma diversificata di tipi di oggetti che non apparirebbero mai in scenari di guida urbana: animali (elefante, giraffa, cane, gatto, orso), mobili (sedia, divano, letto, tavolo da pranzo), elettronica (laptop, tastiera, cellulare, telecomando), oggetti alimentari (pizza, torta, banana, mela), attrezzature sportive (mazza da baseball, racchetta da tennis, skateboard), e molti altri. Questa diversità è cruciale per addestrare il modello a riconoscere anomalie di varie forme, dimensioni, texture e contenuto semantico.

\subsubsection{Struttura Output}

Lo script di download produce una struttura di directory attentamente organizzata, ottimizzata per la successiva fase di generazione Cut-Paste:
\begin{lstlisting}
coco_ood_subset/
    images/          # Immagini RGB ritagliate di singoli oggetti OOD
    masks/           # Maschere binarie di istanza (255=oggetto, 0=sfondo)
    metadata.json    # JSON con info oggetto: bbox, category_id,
                     # category_name, original_image_id, area
\end{lstlisting}

La directory \texttt{images/} contiene patch RGB ritagliate strettamente centrate su ogni istanza di oggetto, estratte usando le annotazioni bounding box. La directory \texttt{masks/} memorizza le maschere binarie corrispondenti dove il valore pixel 255 indica la regione oggetto e 0 indica lo sfondo. Queste maschere sono derivate dalle annotazioni poligonali di COCO, rasterizzate per corrispondere alle dimensioni del ritaglio. Il file \texttt{metadata.json} mantiene un record completo delle proprietà di ogni oggetto, permettendo filtraggi a valle per dimensione, categoria o altri criteri se necessario.

\subsection{Step 2: Generazione Dataset Cut-Paste (\texttt{generate\_cutpaste\_dataset.py})}

Lo script \texttt{generate\_cutpaste\_dataset.py} è il cuore della nostra pipeline di augmentation. Prende il training set originale Cityscapes e gli oggetti OOD COCO scaricati, e produce un nuovo dataset dove oggetti anomali sono stati inseriti sinteticamente nelle scene di guida urbana.

Abbiamo scelto una strategia di \textbf{augmentation offline} piuttosto che applicare Cut-Paste on-the-fly durante il training. Questa scelta progettuale è stata guidata da diverse considerazioni pratiche:
\begin{itemize}
    \item \textbf{Velocità Training}: L'augmentation in tempo reale aggiungerebbe overhead computazionale ad ogni iterazione di training. Caricare immagini pre-augmentate è veloce quanto caricare immagini originali, evitando qualsiasi rallentamento nel loop di training.
    \item \textbf{Riproducibilità}: Generando il dataset augmentato una volta, garantiamo che ogni run di training veda esattamente le stesse immagini augmentate. Questo elimina una fonte di casualità e rende gli esperimenti più riproducibili.
    \item \textbf{Ispezione Visiva}: Prima di impegnarsi in un training completo (che può richiedere ore o giorni), possiamo ispezionare visivamente le immagini generate per verificare che l'augmentation appaia realistica. Questo permette di individuare errori precocemente.
    \item \textbf{Debugging}: Se il modello fallisce nel rilevare anomalie, possiamo esaminare le immagini di training per determinare se il problema risiede nel processo di augmentation o nel modello stesso.
\end{itemize}

Lo script è implementato come classe Python \texttt{CutPasteGenerator} che incapsula tutta la logica di augmentation. Usa multiprocessing per parallelizzare la generazione tra i core CPU, riducendo significativamente il tempo totale di generazione per le 2.975 immagini di training Cityscapes.

\subsubsection{Pipeline di Augmentation}

La pipeline di augmentation processa ciascuna delle 2.975 immagini di training Cityscapes secondo la seguente procedura:

\textbf{Step A: Decisione di Augmentation.} Per ogni immagine, un lancio di moneta con probabilità $p = 0.5$ determina se l'augmentation Cut-Paste verrà applicata. Questo significa che circa metà delle immagini di training conterranno anomalie sintetiche, mentre l'altra metà rimarrà invariata. Questo bilanciamento è importante: se tutte le immagini contenessero anomalie, il modello potrebbe imparare a predire sempre anomalie; al contrario, troppo poche immagini con anomalie fornirebbero supervisione insufficiente per la classe anomalia.

\textbf{Step B: Selezione Oggetti.} Se l'augmentation viene applicata, selezioniamo casualmente $n \sim \mathcal{U}(1, 3)$ oggetti dal pool COCO OOD. La distribuzione uniforme assicura varietà nella complessità della scena---alcune immagini avranno una singola anomalia, altre ne avranno fino a tre. Gli oggetti vengono selezionati completamente a caso da tutte le categorie disponibili, promuovendo diversità nell'aspetto delle anomalie.

\textbf{Step C: Processing Oggetto.} Per ogni oggetto selezionato, eseguiamo le seguenti trasformazioni:
\begin{enumerate}
    \item \textbf{Caricamento}: Lettura del crop RGB dell'oggetto e della maschera binaria da disco.
    \item \textbf{Scaling}: Applicazione di un fattore di scala casuale $s \sim \mathcal{U}(0.5, 1.5)$ sia al crop che alla maschera. Per prevenire oggetti irrealisticamente grandi, limitiamo la dimensione finale al 40\% delle dimensioni dell'immagine target. Gli oggetti che eccederebbero questo limite vengono ridimensionati proporzionalmente.
    \item \textbf{Posizionamento}: Selezione di una posizione casuale $(x, y)$ sull'immagine target. La posizione viene scelta uniformemente a caso, permettendo agli oggetti di apparire ovunque incluso vicino ai bordi dell'immagine. Gli oggetti che si estendono oltre i confini dell'immagine vengono semplicemente tagliati.
    \item \textbf{Blending}: Applicazione di alpha compositing usando la maschera sfumata (dettagliata sotto) per fondere seamlessly l'oggetto con lo sfondo.
    \item \textbf{Aggiornamento Label}: Nel ground truth di segmentazione, tutti i pixel coperti dall'oggetto (dove maschera $> 0.5$) vengono assegnati allo speciale anomaly label ID = 254.
\end{enumerate}

\subsubsection{Tecnica Alpha Feathering}

Un'implementazione naïve del Cut-Paste semplicemente sovrascriverebbe i pixel dello sfondo con i pixel dell'oggetto ovunque la maschera binaria sia uguale a 1. Tuttavia, questo crea bordi innaturalmente netti ai confini dell'oggetto---bordi che non si verificherebbero mai in fotografie reali. Un modello addestrato su tali dati potrebbe imparare a rilevare anomalie cercando questi artefatti di bordo artificiali piuttosto che imparando genuine caratteristiche delle anomalie. Questo porterebbe a scarsa generalizzazione sui benchmark di anomalie reali dove gli oggetti hanno bordi naturali.

Per affrontare questo problema, implementiamo una \textbf{tecnica di alpha feathering} che crea transizioni morbide e graduali tra gli oggetti incollati e lo sfondo. La tecnica funziona convertendo la maschera binaria in un canale alpha continuo con valori che transizionano dolcemente da 1 (completamente opaco) nell'interno dell'oggetto a 0 (completamente trasparente) ai bordi.

\begin{algorithm}
\caption{Creazione Maschera Sfumata}
\begin{algorithmic}[1]
\State $M_{float} \gets M_{binary} / 255.0$ \Comment{Normalizza a [0, 1]}
\State $M_{eroded} \gets \text{Erode}(M_{float}, \text{kernel}=3\times3, \text{iterations}=2)$
\State $M_{blurred} \gets \text{GaussianBlur}(M_{float}, \sigma=5)$
\State $M_{feathered} \gets \max(M_{eroded}, M_{blurred})$
\State \Return $M_{feathered}$
\end{algorithmic}
\end{algorithm}

L'algoritmo combina due operazioni complementari:
\begin{itemize}
    \item \textbf{Erosione} (kernel $3\times3$, 2 iterazioni): L'erosione morfologica restringe la maschera verso l'interno, rimuovendo circa 6 pixel dal confine. Questo crea una regione di \textbf{nucleo solido} dove $\alpha = 1$, assicurando che la parte centrale dell'oggetto sia renderizzata a piena opacità. Senza erosione, un blur eccessivo potrebbe far apparire gli oggetti piccoli come fantasmi.
    \item \textbf{Gaussian Blur} ($\sigma = 5$): Sfumare la maschera binaria originale crea un gradiente morbido da 1 dentro l'oggetto a 0 fuori. La larghezza del gradiente è controllata da $\sigma$; valori più grandi creano transizioni più ampie e morbide.
\end{itemize}

L'operazione $\max$ combina queste due maschere: la maschera erosa garantisce un centro solido, mentre la maschera sfumata fornisce \textbf{bordi morbidi}. Il risultato è una maschera che transiziona dolcemente dall'interno opaco all'esterno trasparente su una banda di circa 10-15 pixel.

Il blending finale segue la formula standard di alpha compositing Porter-Duff:
\begin{equation}
    I_{output}(x,y) = \alpha(x,y) \cdot I_{object}(x,y) + (1 - \alpha(x,y)) \cdot I_{background}(x,y)
\end{equation}

Questa interpolazione pixel per pixel assicura che i pixel ai bordi contengano una miscela pesata dei colori dell'oggetto e dello sfondo, simulando l'anti-aliasing naturale che si verifica nelle fotografie reali.

\subsubsection{Struttura Output}

Lo script produce una struttura di directory compatibile con Cityscapes:
\begin{lstlisting}
cityscapes_cutpaste/
    leftImg8bit/
        train/
            {city}/
                {city}_XXXXXX_YYYYYY_leftImg8bit.png
    gtFine/
        train/
            {city}/
                {city}_XXXXXX_YYYYYY_gtFine_labelIds.png
    metadata_train.json   # Statistiche e lista file
\end{lstlisting}

\subsection{Step 3: Packaging Dataset (\texttt{zip\_dataset.py})}

Il dataset Cut-Paste generato può essere piuttosto grande: ogni immagine Cityscapes è 2048$\times$1024 pixel, e con 2.975 immagini di training più le corrispondenti mappe di etichette, il dataset non compresso supera i 10 GB. Trasferire dataset così grandi tra macchine---per esempio, da una workstation locale dove è stata eseguita l'augmentation a un cluster GPU cloud per il training---può essere dispendioso in termini di tempo e costi.

Lo script \texttt{zip\_dataset.py} affronta questa esigenza pratica comprimendo l'intero dataset generato in un singolo archivio ZIP usando l'algoritmo di compressione \texttt{ZIP\_DEFLATED}. Questo metodo di compressione standard raggiunge buoni rapporti di compressione sulle immagini PNG mantenendo ampia compatibilità tra sistemi operativi.

Lo script implementa diverse funzionalità per robustezza:
\begin{itemize}
    \item \textbf{Tracking Progresso}: Una barra di progresso mostra l'avanzamento della compressione per dataset grandi
    \item \textbf{Preservazione Struttura Directory}: L'archivio ZIP mantiene l'esatta gerarchia di directory compatibile con Cityscapes, così l'estrazione produce un dataset pronto all'uso
    \item \textbf{Compressione Selettiva}: Solo i file necessari (immagini ed etichette) sono inclusi; file temporanei e cache sono esclusi
\end{itemize}

Il file ZIP risultante può essere caricato su storage cloud, trasferito via rete, o archiviato per scopi di riproducibilità.

\subsection{Step 4: Configurazione Training (\texttt{eomt\_base\_640\_cutpaste.yaml})}

Il framework EoMT utilizza file di configurazione YAML per specificare tutti gli iperparametri di training, l'architettura del modello e le impostazioni della pipeline dati. Per il training Cut-Paste, abbiamo creato un nuovo file di configurazione \texttt{eomt\_base\_640\_cutpaste.yaml} che estende il setup standard di segmentazione semantica EoMT con le modifiche necessarie per il training anomaly-aware.

I cambiamenti più significativi sono:

\begin{lstlisting}[language=yaml]
model:
  init_args:
    num_classes: 20  # 19 Cityscapes + 1 anomalia
    load_ckpt_class_head: false  # Reinizializza classificatore

data:
  class_path: datasets.cityscapes_semantic_cutpaste.CityscapesSemanticCutPaste
  init_args:
    path: "/path/to/cityscapes_cutpaste"
    original_cityscapes_path: "/path/to/original/cityscapes"
\end{lstlisting}

\textbf{Numero di Classi}: La configurazione standard Cityscapes usa \texttt{num\_classes: 19}, corrispondente alle 19 classi di valutazione nel benchmark Cityscapes. Aumentiamo questo a \texttt{num\_classes: 20} per aggiungere la classe anomalia come 20° canale di output. Questo cambiamento si propaga attraverso l'intera architettura del modello: la classification head finale ora produce 20 logit per pixel invece di 19.

\textbf{Reinizializzazione Classification Head}: Il parametro \texttt{load\_ckpt\_class\_head: false} è critico per un training corretto. Partiamo da un checkpoint EoMT preaddestrato che è stato trainato su Cityscapes standard a 19 classi. La classification head in questo checkpoint ha forma \texttt{[hidden\_dim, 19]}. Se tentassimo di caricare questi pesi in un modello a 20 classi (forma \texttt{[hidden\_dim, 20]}), otterremmo un errore di mismatch dimensionale. Impostando questo parametro a \texttt{false}, istruiamo lo script di training a saltare il caricamento dei pesi della classification head e invece inizializzare casualmente la nuova head a 20 classi. Il resto del modello (backbone, layer transformer, positional embedding) viene comunque caricato dal checkpoint preaddestrato, preservando le rappresentazioni visive apprese.

\textbf{Data Module Custom}: Specifichiamo il nostro data module custom \texttt{CityscapesSemanticCutPaste}, che sa come caricare il dataset augmentato e parsare correttamente le etichette anomalia. La configurazione fornisce due percorsi: il percorso al dataset augmentato Cut-Paste (usato per il training) e il percorso al Cityscapes originale (usato per la validazione).

\subsection{Step 5: DataModule Custom (\texttt{cityscapes\_semantic\_cutpaste.py})}

L'ultimo pezzo della pipeline è il DataModule PyTorch Lightning che integra il dataset augmentato Cut-Paste con il framework di training EoMT. Questo modulo estende il DataModule standard di segmentazione semantica Cityscapes con logica custom per gestire la classe anomalia.

Il DataModule è responsabile di:
\begin{itemize}
    \item Caricare immagini e mappe di etichette dalle directory corrette
    \item Applicare augmentation al tempo di training (crop casuali, flip, jitter colore)
    \item Parsare le mappe di etichette nel formato atteso dalla loss function di EoMT
    \item Creare oggetti DataLoader con batch size e worker appropriati
\end{itemize}

\subsubsection{Parsing Target}

La personalizzazione più significativa è il metodo \texttt{target\_parser}, che converte le mappe di etichette raw nel formato basato su maschere usato dalla mask classification loss di EoMT. La sfida chiave è gestire correttamente l'anomaly label ID = 254, che non esiste nello schema standard delle etichette Cityscapes.

\begin{lstlisting}[language=Python]
def target_parser(target, anomaly_label_id=254, **kwargs):
    masks, labels = [], []
    for label_id in target[0].unique():
        if label_id == anomaly_label_id:
            # Gestione speciale per pixel anomalia
            masks.append(target[0] == label_id)
            labels.append(19)  # Classe anomalia = train_id 19
            continue
        # Gestione classi Cityscapes standard:
        # Converti Cityscapes label_id a train_id...
    return masks, labels, is_crowd
\end{lstlisting}

La logica di parsing itera su tutti gli ID di etichetta unici presenti nel ground truth. Quando incontra il valore speciale 254, crea una maschera binaria per quei pixel e li assegna alla classe 19 (la classe anomalia, che è la 20ª classe con indicizzazione 0-based). Tutti gli altri ID di etichetta vengono processati usando la mappatura standard Cityscapes label-to-trainId.

Questa scelta progettuale---usare label ID 254 nei file raw e mapparlo a train\_id 19 a runtime---mantiene la compatibilità con il formato file Cityscapes aggiungendo supporto per le anomalie.

\subsubsection{Struttura Dataset}

Il DataModule implementa un'asimmetria cruciale tra training e validazione:
\begin{itemize}
    \item \textbf{Training Set}: Usa le immagini pre-generate con Cut-Paste applicato da \texttt{cityscapes\_cutpaste/}. Queste immagini possono contenere oggetti OOD incollati etichettati con ID 254.
    \item \textbf{Validation Set}: Usa le immagini di validazione Cityscapes originali, non modificate, dalla posizione standard del dataset. Queste immagini contengono solo le 19 classi standard senza anomalie.
\end{itemize}

Questa separazione serve due scopi:
\begin{enumerate}
    \item \textbf{Valutazione Fair}: La mIoU di validazione è calcolata su dati Cityscapes puliti, permettendo confronto diretto con i modelli baseline. Se anche la validazione contenesse anomalie sintetiche, la metrica mIoU sarebbe artificialmente influenzata.
    \item \textbf{Test di Generalizzazione}: Il modello impara a rilevare anomalie da oggetti COCO ma viene valutato su benchmark di anomalie reali (RoadAnomaly, ecc.) con tipi di anomalie completamente diversi. Questo testa la vera generalizzazione piuttosto che la memorizzazione.
\end{enumerate}


\section{Modifiche al Modello}

Mentre l'architettura core di EoMT rimane invariata, diverse modifiche all'infrastruttura di training sono state necessarie per supportare il training Cut-Paste. Questi cambiamenti sono implementati nel modulo Lightning (\texttt{lightning\_module.py}) e gestiscono problemi di compatibilità durante il caricamento di checkpoint preaddestrati.

\subsection{Modifiche Lightning Module (\texttt{lightning\_module.py})}

Il modulo PyTorch Lightning orchestra il loop di training, il caricamento checkpoint e il logging. Due modifiche chiave sono state fatte per gestire i requisiti specifici del nostro setup di training Cut-Paste:

\subsubsection{Interpolazione Positional Embedding}

I Vision Transformer (ViT), incluso il backbone DINOv2 usato in EoMT, utilizzano positional embedding appresi per codificare l'informazione spaziale. Questi embedding sono addestrati a una risoluzione specifica e hanno una dimensione spaziale fissa. Per esempio, un modello addestrato a risoluzione 1024$\times$1024 con patch size di 16 ha positional embedding di forma $(64 \times 64) = 4096$ token.

Quando vogliamo addestrare a una risoluzione diversa (es. 640$\times$640, che dà $(40 \times 40) = 1600$ token), non possiamo usare direttamente i positional embedding preaddestrati perché le dimensioni non corrispondono. Semplicemente troncare o fare zero-padding distruggerebbe le relazioni spaziali apprese.

La nostra soluzione è l'\textbf{interpolazione bicubica} dei positional embedding:

\begin{equation}
    \text{pos\_embed}_{new} = \text{Interpolate}_{bicubic}\left(\text{pos\_embed}_{ckpt}, \left(\frac{H_{new}}{16}, \frac{W_{new}}{16}\right)\right)
\end{equation}

Il processo di interpolazione:
\begin{enumerate}
    \item Reshape dei positional embedding 1D a griglia spaziale 2D (escludendo il token CLS se presente)
    \item Applicazione di interpolazione bicubica per ridimensionare la griglia alle nuove dimensioni spaziali
    \item Flatten di nuovo a 1D e concatenazione con l'embedding del token CLS (invariato)
\end{enumerate}

L'interpolazione bicubica è preferita rispetto alla bilineare perché produce risultati più smooth con meno aliasing. Questa tecnica preserva la codifica spaziale relativa appresa durante il pretraining adattandosi alla nuova risoluzione.

\subsubsection{Purificazione Parametri}

Certe operazioni di deep learning, particolarmente quelle che coinvolgono trasformate di Fourier (FFT), possono produrre tensori a valori complessi con componenti reali e immaginarie. Mentre questi sono matematicamente validi durante i forward pass, possono causare problemi durante il salvataggio/caricamento dei checkpoint e il calcolo dei gradienti.

Abbiamo incontrato checkpoint dove alcuni parametri erano diventati inavvertitamente a valori complessi (\texttt{torch.complex64}). Per assicurare stabilità di training e compatibilità, implementiamo uno step di \textbf{purificazione parametri} che forza tutti i parametri a float32 a valori reali:

\begin{lstlisting}[language=Python]
def _purify_parameters(module):
    """Converti qualsiasi parametro complesso a float32 reale."""
    for name, param in module.named_parameters():
        if torch.is_complex(param):
            # Estrai parte reale e crea nuovo parametro
            new_data = param.data.real.detach().clone().float()
            # Sostituisci il parametro nel modulo
            setattr(module, param_name, nn.Parameter(new_data))
\end{lstlisting}

Questa funzione attraversa tutti i parametri nel modello. Per ogni parametro con dtype complesso, estrae la componente reale, converte a float32, e sostituisce il parametro originale. Questo assicura che l'intero modello usi parametri a valori reali consistenti adatti all'ottimizzazione standard con gradient descent.


\section{Metriche di Valutazione}

Per valutare il nostro modello trainato con Cut-Paste, abbiamo sviluppato uno script di valutazione custom (\texttt{evalAnomaly\_cut\_paste.py}) che calcola sia le metriche tradizionali di rilevamento anomalie che la nostra nuova metrica learned anomaly. Lo script processa benchmark standard di segmentazione anomalie e produce report di performance completi.

\subsection{Metriche Standard (Normalizzate su 19 Classi)}

Per un confronto equo con i metodi baseline che sono stati trainati su Cityscapes standard a 19 classi, calcoliamo gli score di anomalia tradizionali basati sull'incertezza usando solo i primi 19 logit di output. Questo assicura che stiamo confrontando mele con mele: i metodi baseline vedono solo 19 classi, quindi normalizziamo le predizioni del nostro modello allo stesso spazio a 19 classi quando calcoliamo queste metriche.

Data un'immagine di input, sia $f_c(x,y)$ il logit (attivazione pre-softmax) per la classe $c$ alla posizione pixel $(x,y)$, e sia $P(c|x,y) = \text{softmax}(f)_c$ la probabilità corrispondente. Le metriche standard sono:

\begin{itemize}
    \item \textbf{MSP (Maximum Softmax Probability)}: 
    \begin{equation}
        \text{MSP}(x,y) = 1 - \max_{c \in \{1,...,19\}} P(c|x,y)
    \end{equation}
    L'intuizione è che i pixel in-distribution dovrebbero avere alta confidenza per una delle classi note, mentre i pixel OOD dovrebbero avere probabilità massima più bassa. Usiamo $1 - \max P$ così che score più alti indicano maggiore likelihood di anomalia.
    
    \item \textbf{MaxLogit}: 
    \begin{equation}
        \text{MaxLogit}(x,y) = -\max_{c \in \{1,...,19\}} f_c(x,y)
    \end{equation}
    Simile a MSP ma opera sui logit raw piuttosto che sulle probabilità. La negazione assicura che score più alti indicano anomalie. MaxLogit è spesso preferito a MSP perché la softmax può saturare e nascondere informazioni utili nei logit.
    
    \item \textbf{Entropy}: 
    \begin{equation}
        \text{Entropy}(x,y) = -\sum_{c=1}^{19} P(c|x,y) \log P(c|x,y)
    \end{equation}
    Misura l'incertezza nella distribuzione di probabilità. I pixel in-distribution dovrebbero avere distribuzioni piccate (bassa entropia) mentre i pixel OOD dovrebbero avere distribuzioni più uniformi (alta entropia).
    
    \item \textbf{RbA (Residual-based Anomaly)}: 
    \begin{equation}
        \text{RbA}(x,y) = 1 - \sum_{c=1}^{19} P(c|x,y)
    \end{equation}
    Questa metrica sfrutta il fatto che le probabilità softmax devono sommare a 1. Quando normalizzato solo sulle 19 classi (ignorando la classe 20), la somma può essere minore di 1 se il modello assegna probabilità alla 20ª classe. Tuttavia, per modelli standard a 19 classi, questo equivale a zero ovunque.
\end{itemize}

\subsection{Metrica Learned Anomaly}

Il vantaggio chiave del nostro training Cut-Paste è abilitare una nuova metrica che usa direttamente la classe anomalia appresa:
\begin{equation}
    \text{Learned\_Anomaly}(x,y) = P(\text{class}=20 | x, y) = \text{softmax}(f)_{20}
\end{equation}

A differenza delle metriche basate sull'incertezza sopra, che sono misure \emph{indirette} della likelihood di anomalia (basate sul modello che è incerto o ha bassa confidenza), lo score Learned\_Anomaly è una predizione \emph{diretta}. Il modello produce esplicitamente quanto è probabile che ogni pixel sia un'anomalia, trainato con supervisione diretta dal ground truth augmentato Cut-Paste.

Per calcolare questa metrica, applichiamo softmax su tutti i 20 logit (non solo i primi 19) ed estraiamo la probabilità assegnata alla classe 20 (indice 19 in indicizzazione 0-based). Questa probabilità risponde direttamente alla domanda: ``Il modello pensa che questo pixel sia un'anomalia?''

\subsection{Metriche Benchmark}

Per ogni score di anomalia (MSP, MaxLogit, Entropy, RbA, Learned\_Anomaly), calcoliamo due metriche di valutazione standard confrontando con le maschere ground truth di anomalia:

\begin{itemize}
    \item \textbf{AUPRC} (Area Under Precision-Recall Curve): La curva precision-recall traccia la precisione contro il recall a varie impostazioni di soglia. AUPRC riassume questa curva come un singolo numero tra 0 e 1. Questa metrica è particolarmente appropriata per il rilevamento anomalie perché:
    \begin{itemize}
        \item Le anomalie sono tipicamente rare (sbilanciamento di classe), e AUPRC è più informativa di ROC-AUC in setting sbilanciati
        \item Un classificatore random raggiunge AUPRC uguale alla proporzione della classe positiva (spesso molto bassa), rendendo i miglioramenti più significativi
        \item AUPRC più alto indica migliore qualità di rilevamento complessiva
    \end{itemize}
    
    \item \textbf{FPR@95} (False Positive Rate al 95\% True Positive Rate): Questa metrica risponde a una domanda pratica: ``Se vogliamo rilevare il 95\% di tutte le anomalie, quanti falsi allarmi avremo?'' 
    \begin{itemize}
        \item Troviamo la soglia che raggiunge 95\% true positive rate (rilevando il 95\% dei pixel anomalia effettivi)
        \item A questa soglia, misuriamo quale frazione di pixel normali viene erroneamente segnalata come anomalie
        \item FPR@95 più basso indica meno falsi allarmi mantenendo alto rate di rilevamento
    \end{itemize}
\end{itemize}


\section{Risultati Sperimentali}

Valutiamo il nostro modello EoMT trainato con Cut-Paste su cinque benchmark standard di segmentazione anomalie. Questi benchmark contengono immagini del mondo reale con oggetti anomali genuini (non incollati sinteticamente), fornendo un vero test di generalizzazione dai dati di training sintetici alle anomalie reali.

\subsection{Benchmark di Valutazione}

\begin{itemize}
    \item \textbf{RoadAnomaly}: Un benchmark focalizzato su ostacoli stradali diversi e oggetti insoliti incontrati durante la guida. Contiene 60 immagini con annotazioni anomalia a livello di pixel.
    
    \item \textbf{RoadAnomaly21}: Una versione aggiornata di RoadAnomaly con immagini aggiuntive e annotazioni migliorate. Parte della suite benchmark SegmentMeIfYouCan.
    
    \item \textbf{RoadObsticle21}: Si concentra specificamente sugli ostacoli sulla superficie stradale (detriti, carico perso, animali) che un veicolo deve rilevare ed evitare. Anche parte di SegmentMeIfYouCan.
    
    \item \textbf{FS\_LostFound\_full}: Il benchmark Fishyscapes Lost\&Found, contenente immagini dal dataset Lost\&Found con piccoli ostacoli sulle strade.
    
    \item \textbf{fs\_static}: Benchmark Fishyscapes Static dove oggetti anomali sono compositi sinteticamente nelle immagini di validazione Cityscapes. Sebbene sintetico, fornisce un setting di valutazione controllato.
\end{itemize}

La Tabella \ref{tab:results_it} mostra il confronto delle performance attraverso questi benchmark.

\begin{table}[h]
\centering
\caption{Risultati Segmentazione Anomalie (AUPRC \% / FPR@95 \%)}
\label{tab:results_it}
\begin{tabular}{l|cc|cc|cc}
\toprule
\textbf{Dataset} & \multicolumn{2}{c|}{\textbf{MaxLogit}} & \multicolumn{2}{c|}{\textbf{Entropy}} & \multicolumn{2}{c}{\textbf{Learned\_Anomaly}} \\
& AUPRC & FPR & AUPRC & FPR & AUPRC & FPR \\
\midrule
RoadAnomaly & 15.54 & 71.19 & 21.72 & 91.34 & \textbf{25.11} & 91.34 \\
RoadAnomaly21 & 22.49 & 76.36 & 21.06 & 95.42 & \textbf{25.17} & 95.42 \\
RoadObsticle21 & 10.75 & 56.73 & 6.90 & 75.42 & \textbf{25.56} & 57.74 \\
FS\_LostFound & 0.37 & 74.33 & 0.34 & 96.40 & \textbf{0.47} & 96.40 \\
fs\_static & 2.23 & 84.11 & 2.19 & 95.26 & \textbf{2.88} & 95.26 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analisi}

La metrica Learned\_Anomaly supera consistentemente i metodi tradizionali basati sull'incertezza in termini di AUPRC attraverso tutti i benchmark:

\begin{itemize}
    \item \textbf{RoadObsticle21}: Il miglioramento più drammatico si osserva qui, con AUPRC che aumenta da 10.75\% (MaxLogit) a 25.56\% (+137\% miglioramento relativo). Questo benchmark contiene ostacoli chiaramente out-of-distribution (animali, detriti), che il modello ha imparato a riconoscere esplicitamente attraverso il training Cut-Paste.
    
    \item \textbf{RoadAnomaly}: AUPRC migliora da 21.72\% (Entropy) a 25.11\% (+16\% miglioramento relativo). Il miglioramento è più modesto qui, probabilmente perché le anomalie in questo benchmark sono più diverse e challenging.
    
    \item \textbf{FS\_LostFound e fs\_static}: Mentre i numeri assoluti sono più bassi (questi benchmark contengono ostacoli piccoli e challenging), la metrica Learned\_Anomaly raggiunge comunque la migliore AUPRC tra tutti i metodi.
\end{itemize}

\subsection{Discussione}

Questi risultati dimostrano diverse scoperte chiave:

\begin{enumerate}
    \item \textbf{La supervisione esplicita aiuta}: Nonostante sia stato trainato su oggetti COCO sintetici incollati su Cityscapes, il modello generalizza con successo per rilevare anomalie reali di tipi completamente diversi. Questo valida la nostra strategia Cut-Paste.
    
    \item \textbf{Esiste un domain gap}: I miglioramenti, sebbene significativi, non sono uniformi attraverso i benchmark. Alcuni tipi di anomalie potrebbero essere più simili agli oggetti COCO di training rispetto ad altri, portando a performance variabili.
    
    \item \textbf{Trade-off FPR@95}: La metrica Learned\_Anomaly mostra risultati misti su FPR@95. In alcuni casi (RoadObsticle21), mantiene FPR competitivo migliorando drasticamente AUPRC. In altri casi, FPR aumenta. Questo suggerisce potenziale per tuning della soglia basato sui requisiti dell'applicazione.
    
    \item \textbf{Preservazione baseline}: Importante, le nostre modifiche non degradano le performance di segmentazione standard. Il modello raggiunge comunque mIoU comparabile sulla validazione Cityscapes rispetto alla baseline (non mostrato in questa tabella), dimostrando che imparare a rilevare anomalie non ha un costo sul riconoscimento delle classi note.
\end{enumerate}


\end{document}
