{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Install requirements\n",
        "!pip install ood-metrics jsonargparse pytorch-lightning gitignore_parser\n",
        "!pip install -r AnomalySegmentation/eomt/requirements.txt\n",
        "!pip install \"jsonargparse[signatures]\"\n",
        "!pip install lightning\n",
        "\n",
        "# 3. Clone repo\n",
        "REPO_URL = \"https://github.com/MatteoSanna23/AnomalySegmentation.git\"\n",
        "REPO_DIR = \"AnomalySegmentation\""
      ],
      "metadata": {
        "id": "SGZ35th3Rj2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff07bfe-38a0-4ece-fa88-485cc70b70a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: ood-metrics in /usr/local/lib/python3.12/dist-packages (0.3.0)\n",
            "Requirement already satisfied: jsonargparse in /usr/local/lib/python3.12/dist-packages (4.45.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: gitignore_parser in /usr/local/lib/python3.12/dist-packages (0.1.13)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from ood-metrics) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.12/dist-packages (from ood-metrics) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from ood-metrics) (1.6.1)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.12/dist-packages (from jsonargparse) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Requirement already satisfied: torchmetrics>0.7.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (0.15.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0,>=3.0->ood-metrics) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0,>=1.0->ood-metrics) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0,>=1.0->ood-metrics) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0,>=1.0->ood-metrics) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.20.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib<4.0,>=3.0->ood-metrics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.11)\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'AnomalySegmentation/eomt/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: jsonargparse[signatures] in /usr/local/lib/python3.12/dist-packages (4.45.0)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[signatures]) (6.0.3)\n",
            "Requirement already satisfied: docstring-parser>=0.17 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[signatures]) (0.17.0)\n",
            "Collecting typeshed-client>=2.8.2 (from jsonargparse[signatures])\n",
            "  Downloading typeshed_client-2.8.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: importlib_resources>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from typeshed-client>=2.8.2->jsonargparse[signatures]) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from typeshed-client>=2.8.2->jsonargparse[signatures]) (4.15.0)\n",
            "Downloading typeshed_client-2.8.2-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typeshed-client\n",
            "Successfully installed typeshed-client-2.8.2\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.6.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.12/dist-packages (from lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.3.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (0.15.2)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (25.0)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (1.8.2)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.15.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (from lightning) (2.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.20.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics<3.0,>0.7.0->lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.11)\n",
            "Downloading lightning-2.6.0-py3-none-any.whl (845 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning\n",
            "Successfully installed lightning-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TuqhQoo9PwOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906768d6-e2d3-477f-a75d-d66550f3fe2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Clonazione della repository...\n",
            "Cloning into 'AnomalySegmentation'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 154 (delta 31), reused 22 (delta 17), pack-reused 89 (from 3)\u001b[K\n",
            "Receiving objects: 100% (154/154), 26.89 MiB | 13.32 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(REPO_DIR):\n",
        "    print(\">> Clonazione della repository...\")\n",
        "    !git clone --branch matte {REPO_URL}\n",
        "else:\n",
        "    print(\">> Repository giÃ  presente.\")\n",
        "\n",
        "os.chdir(REPO_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Preparazione Dataset (METODO ISTANTANEO: SYMLINK)\n",
        "DATA_ROOT = \"/content/data/cityscapes\"\n",
        "!mkdir -p {DATA_ROOT}\n",
        "\n",
        "# Percorsi su Drive\n",
        "ZIP_IMG_SOURCE = \"/content/drive/MyDrive/Colab Notebooks/leftImg8bit_trainvaltest.zip\"\n",
        "ZIP_GT_SOURCE = \"/content/drive/MyDrive/Colab Notebooks/gtFine_trainvaltest.zip\"\n",
        "\n",
        "# Percorsi Locali\n",
        "ZIP_IMG_DEST = os.path.join(DATA_ROOT, \"leftImg8bit_trainvaltest.zip\")\n",
        "ZIP_GT_DEST = os.path.join(DATA_ROOT, \"gtFine_trainvaltest.zip\")\n",
        "\n",
        "print(\">> Creazione collegamenti ai dataset...\")\n",
        "\n",
        "# Creiamo il collegamento simbolico invece di copiare\n",
        "if os.path.exists(ZIP_IMG_SOURCE):\n",
        "    # -s crea il link, -f forza la sovrascrittura se esiste giÃ  un file rotto\n",
        "    !ln -sf \"{ZIP_IMG_SOURCE}\" \"{ZIP_IMG_DEST}\"\n",
        "    print(f\">> Link creato per le immagini.\")\n",
        "else:\n",
        "    print(\"!! ERRORE: Non trovo il file zip delle immagini su Drive!\")\n",
        "\n",
        "if os.path.exists(ZIP_GT_SOURCE):\n",
        "    !ln -sf \"{ZIP_GT_SOURCE}\" \"{ZIP_GT_DEST}\"\n",
        "    print(f\">> Link creato per le annotazioni.\")\n",
        "else:\n",
        "    print(\"!! ERRORE: Non trovo il file zip delle annotazioni su Drive!\")\n",
        "\n",
        "print(\">> Contenuto cartella dati (ora dovresti vedere i file in azzurrino/link):\")\n",
        "!ls -lh {DATA_ROOT}"
      ],
      "metadata": {
        "id": "wNGnujDaR7-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4aa315-de6e-42bf-ed09-2e2bb50f6175"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Creazione collegamenti ai dataset...\n",
            ">> Link creato per le immagini.\n",
            ">> Link creato per le annotazioni.\n",
            ">> Contenuto cartella dati (ora dovresti vedere i file in azzurrino/link):\n",
            "total 20K\n",
            "drwxr-xr-x 4 root root 4.0K Jan 19 10:53 gtFine\n",
            "lrwxrwxrwx 1 root root   62 Jan 19 11:10 gtFine_trainvaltest.zip -> '/content/drive/MyDrive/Colab Notebooks/gtFine_trainvaltest.zip'\n",
            "lrwxrwxrwx 1 root root   67 Jan 19 11:10 leftImg8bit_trainvaltest.zip -> '/content/drive/MyDrive/Colab Notebooks/leftImg8bit_trainvaltest.zip'\n",
            "-rw-r--r-- 1 root root 1.7K Feb 17  2016 license.txt\n",
            "-rw-r--r-- 1 root root  298 Feb 20  2016 README\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA6MtTeR3n4q",
        "outputId": "046a8816-ad2b-4fc7-c584-97ce5a044071"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eomt  eval  README.md  trained_models  wandb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# os.chdir(\"/content/AnomalySegmentation\")\n",
        "\n",
        "print(\">>> Avvio del Training\")\n",
        "!touch .gitignore\n",
        "# Note:\n",
        "# - fit: Ã¨ il comando obbligatorio\n",
        "# - config: punta al file base_640 che Ã¨ piÃ¹ leggero per Colab\n",
        "# - data.init_args.path: dice al codice dove abbiamo scompattato i dati\n",
        "# - trainer.max_epochs: metto 2 epoche per testare se tutto funziona velocemente\n",
        "# --data.init_args.batch_size 1: Fondamentale per non finire la memoria GPU\n",
        "# --trainer.accumulate_grad_batches 4: Simula un batch size piÃ¹ grande\n",
        "# --compile_disabled: Risparmia memoria e tempo di avvio disattivando torch.compile\n",
        "\n",
        "!python eomt/main.py fit \\\n",
        "    --config eomt/configs/dinov2/cityscapes/semantic/eomt_base_640.yaml \\\n",
        "    --data.init_args.path \"{DATA_ROOT}\" \\\n",
        "    --data.init_args.num_workers 2 \\\n",
        "    --data.init_args.batch_size 1 \\\n",
        "    --trainer.accumulate_grad_batches 4 \\\n",
        "    --trainer.max_epochs 1 \\\n",
        "    --trainer.limit_train_batches 50 \\\n",
        "    --trainer.limit_val_batches 10 \\\n",
        "    --trainer.accelerator gpu \\\n",
        "    --trainer.devices 1 \\\n",
        "    --compile_disabled"
      ],
      "metadata": {
        "id": "Ea7Doa0sSEe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9afd9322-9867-46a1-8cb1-2b225d20ff6a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Avvio del Training\n",
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n",
            "2026-01-19 11:39:30.500539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768822770.536094   15103 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768822770.548977   15103 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768822770.574892   15103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768822770.574922   15103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768822770.574928   15103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768822770.574931   15103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-19 11:39:30.582087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Seed set to 0\n",
            "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_base_patch14_reg4_dinov2.lvd142m)\n",
            "INFO:timm.models._hub:[timm/vit_base_patch14_reg4_dinov2.lvd142m] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
            "INFO:timm.layers.pos_embed:Resized position embedding: (37, 37) to (64, 64).\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-sanna\u001b[0m (\u001b[33mmatteo-sanna-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run dn07vk9x (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run dn07vk9x (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run dn07vk9x (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m setting up run dn07vk9x (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m setting up run dn07vk9x (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m setting up run dn07vk9x (0.7s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1mwandb/run-20260119_113940-dn07vk9x\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcityscapes_semantic_eomt_base_640\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/matteo-sanna-politecnico-di-torino/eomt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/matteo-sanna-politecnico-di-torino/eomt/runs/dn07vk9x\u001b[0m\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Loading `train_dataloader` to estimate number of stepping batches.\n",
            "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
            "\n",
            "   | Name                     | Type                        | Params | Mode  | FLOPs\n",
            "------------------------------------------------------------------------------------------\n",
            "0  | network                  | EoMT                        | 95.4 M | train | 0    \n",
            "1  | network.encoder          | ViT                         | 88.8 M | train | 0    \n",
            "2  | network.encoder.backbone | VisionTransformer           | 88.8 M | train | 0    \n",
            "3  | network.q                | Embedding                   | 76.8 K | train | 0    \n",
            "4  | network.class_head       | Linear                      | 15.4 K | train | 0    \n",
            "5  | network.mask_head        | Sequential                  | 1.8 M  | train | 0    \n",
            "6  | network.mask_head.0      | Linear                      | 590 K  | train | 0    \n",
            "7  | network.mask_head.1      | GELU                        | 0      | train | 0    \n",
            "8  | network.mask_head.2      | Linear                      | 590 K  | train | 0    \n",
            "9  | network.mask_head.3      | GELU                        | 0      | train | 0    \n",
            "10 | network.mask_head.4      | Linear                      | 590 K  | train | 0    \n",
            "11 | network.upscale          | Sequential                  | 4.7 M  | train | 0    \n",
            "12 | network.upscale.0        | ScaleBlock                  | 2.4 M  | train | 0    \n",
            "13 | network.upscale.1        | ScaleBlock                  | 2.4 M  | train | 0    \n",
            "14 | criterion                | MaskClassificationLoss      | 0      | train | 0    \n",
            "15 | criterion.matcher        | Mask2FormerHungarianMatcher | 0      | train | 0    \n",
            "16 | metrics                  | ModuleList                  | 0      | train | 0    \n",
            "17 | metrics.0                | MulticlassJaccardIndex      | 0      | train | 0    \n",
            "18 | metrics.1                | MulticlassJaccardIndex      | 0      | train | 0    \n",
            "19 | metrics.2                | MulticlassJaccardIndex      | 0      | train | 0    \n",
            "20 | metrics.3                | MulticlassJaccardIndex      | 0      | train | 0    \n",
            "------------------------------------------------------------------------------------------\n",
            "95.4 M    Trainable params\n",
            "0         Non-trainable params\n",
            "95.4 M    Total params\n",
            "381.662   Total estimated model params size (MB)\n",
            "304       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "0         Total Flops\n",
            "\u001b[2KEpoch 0/0  \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 1/50 \u001b[2m0:00:02 â€¢ -:--:--\u001b[0m \u001b[2;4m0.00it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 2/50 \u001b[2m0:00:03 â€¢ 0:00:18\u001b[0m \u001b[2;4m2.76it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 3/50 \u001b[2m0:00:04 â€¢ 0:00:25\u001b[0m \u001b[2;4m1.89it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 4/50 \u001b[2m0:00:05 â€¢ 0:00:34\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 5/50 \u001b[2m0:00:05 â€¢ 0:00:28\u001b[0m \u001b[2;4m1.62it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 6/50 \u001b[2m0:00:06 â€¢ 0:00:29\u001b[0m \u001b[2;4m1.57it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 7/50 \u001b[2m0:00:07 â€¢ 0:00:28\u001b[0m \u001b[2;4m1.54it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 8/50 \u001b[2m0:00:07 â€¢ 0:00:31\u001b[0m \u001b[2;4m1.36it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 9/50 \u001b[2m0:00:08 â€¢ 0:00:28\u001b[0m \u001b[2;4m1.47it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 10/50 \u001b[2m0:00:09 â€¢ 0:00:28\u001b[0m \u001b[2;4m1.44it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 11/50 \u001b[2m0:00:10 â€¢ 0:00:27\u001b[0m \u001b[2;4m1.45it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 12/50 \u001b[2m0:00:10 â€¢ 0:00:28\u001b[0m \u001b[2;4m1.39it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 13/50 \u001b[2m0:00:11 â€¢ 0:00:26\u001b[0m \u001b[2;4m1.46it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 14/50 \u001b[2m0:00:12 â€¢ 0:00:25\u001b[0m \u001b[2;4m1.45it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m 15/50 \u001b[2m0:00:12 â€¢ 0:00:25\u001b[0m \u001b[2;4m1.45it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m 16/50 \u001b[2m0:00:13 â€¢ 0:00:25\u001b[0m \u001b[2;4m1.38it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m 17/50 \u001b[2m0:00:14 â€¢ 0:00:24\u001b[0m \u001b[2;4m1.42it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m 18/50 \u001b[2m0:00:15 â€¢ 0:00:23\u001b[0m \u001b[2;4m1.41it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m 19/50 \u001b[2m0:00:15 â€¢ 0:00:23\u001b[0m \u001b[2;4m1.40it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m 20/50 \u001b[2m0:00:16 â€¢ 0:00:23\u001b[0m \u001b[2;4m1.35it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m 21/50 \u001b[2m0:00:17 â€¢ 0:00:21\u001b[0m \u001b[2;4m1.38it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m 22/50 \u001b[2m0:00:18 â€¢ 0:00:21\u001b[0m \u001b[2;4m1.38it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m 23/50 \u001b[2m0:00:19 â€¢ 0:00:20\u001b[0m \u001b[2;4m1.38it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m 24/50 \u001b[2m0:00:19 â€¢ 0:00:20\u001b[0m \u001b[2;4m1.34it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m 25/50 \u001b[2m0:00:20 â€¢ 0:00:19\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m 26/50 \u001b[2m0:00:21 â€¢ 0:00:18\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m 27/50 \u001b[2m0:00:22 â€¢ 0:00:17\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m 28/50 \u001b[2m0:00:22 â€¢ 0:00:17\u001b[0m \u001b[2;4m1.34it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m 29/50 \u001b[2m0:00:23 â€¢ 0:00:16\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m 30/50 \u001b[2m0:00:24 â€¢ 0:00:15\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m 31/50 \u001b[2m0:00:25 â€¢ 0:00:14\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m 32/50 \u001b[2m0:00:25 â€¢ 0:00:14\u001b[0m \u001b[2;4m1.35it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m 33/50 \u001b[2m0:00:26 â€¢ 0:00:13\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m 34/50 \u001b[2m0:00:27 â€¢ 0:00:12\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”\u001b[0m 35/50 \u001b[2m0:00:27 â€¢ 0:00:11\u001b[0m \u001b[2;4m1.37it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m 36/50 \u001b[2m0:00:28 â€¢ 0:00:11\u001b[0m \u001b[2;4m1.34it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m 37/50 \u001b[2m0:00:29 â€¢ 0:00:10\u001b[0m \u001b[2;4m1.36it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m 38/50 \u001b[2m0:00:30 â€¢ 0:00:09\u001b[0m \u001b[2;4m1.36it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”\u001b[0m 39/50 \u001b[2m0:00:31 â€¢ 0:00:09\u001b[0m \u001b[2;4m1.35it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”\u001b[0m 40/50 \u001b[2m0:00:32 â€¢ 0:00:08\u001b[0m \u001b[2;4m1.33it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”â”\u001b[0m 41/50 \u001b[2m0:00:32 â€¢ 0:00:07\u001b[0m \u001b[2;4m1.35it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”\u001b[0m 42/50 \u001b[2m0:00:33 â€¢ 0:00:07\u001b[0m \u001b[2;4m1.33it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”\u001b[0m 43/50 \u001b[2m0:00:34 â€¢ 0:00:06\u001b[0m \u001b[2;4m1.35it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”â”\u001b[0m 44/50 \u001b[2m0:00:35 â€¢ 0:00:05\u001b[0m \u001b[2;4m1.31it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m 45/50 \u001b[2m0:00:35 â€¢ 0:00:04\u001b[0m \u001b[2;4m1.33it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m 46/50 \u001b[2m0:00:36 â€¢ 0:00:04\u001b[0m \u001b[2;4m1.32it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m 47/50 \u001b[2m0:00:37 â€¢ 0:00:03\u001b[0m \u001b[2;4m1.35it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m 48/50 \u001b[2m0:00:38 â€¢ 0:00:02\u001b[0m \u001b[2;4m1.31it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[35mâ•¸\u001b[0m 49/50 \u001b[2m0:00:38 â€¢ 0:00:01\u001b[0m \u001b[2;4m1.33it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\u001b[1;32mmIoU: 0.8\u001b[0m\n",
            "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2KEpoch 0/0  \u001b[35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m 50/50 \u001b[2m0:00:39 â€¢ 0:00:00\u001b[0m \u001b[2;4m1.30it/s\u001b[0m \u001b[3mv_num: vk9x       \u001b[0m\n",
            "                                                              \u001b[3mlosses/train_lossâ€¦\u001b[0m\n",
            "                                                              \u001b[3m49.269            \u001b[0m\n",
            "\u001b[?25h\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: ğŸš€ View run \u001b[33mcityscapes_semantic_eomt_base_640\u001b[0m at: \u001b[34m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "loss_file_content =\n",
        "# --------------------------------------------------------------\n",
        "# Â© 2025 Mobile Perception Systems Lab at TU/e. All rights reserved.\n",
        "# Licensed under the MIT License.\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "from typing import List, Optional\n",
        "import torch.distributed as dist\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # needed for F.normalize\n",
        "from transformers.models.mask2former.modeling_mask2former import (\n",
        "    Mask2FormerLoss,\n",
        "    Mask2FormerHungarianMatcher,\n",
        ")\n",
        "\n",
        "class MaskClassificationLoss(Mask2FormerLoss):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_points: int,\n",
        "        oversample_ratio: float,\n",
        "        importance_sample_ratio: float,\n",
        "        mask_coefficient: float,\n",
        "        dice_coefficient: float,\n",
        "        class_coefficient: float,\n",
        "        num_labels: int,\n",
        "        no_object_coefficient: float,\n",
        "        logit_norm_tau: float = 0.1, #! Update 1 : added parameter tau\n",
        "    ):\n",
        "        nn.Module.__init__(self)\n",
        "        self.num_points = num_points\n",
        "        self.oversample_ratio = oversample_ratio\n",
        "        self.importance_sample_ratio = importance_sample_ratio\n",
        "        self.mask_coefficient = mask_coefficient\n",
        "        self.dice_coefficient = dice_coefficient\n",
        "        self.class_coefficient = class_coefficient\n",
        "        self.num_labels = num_labels\n",
        "        self.eos_coef = no_object_coefficient\n",
        "\n",
        "        self.logit_norm_tau = logit_norm_tau #! <--- Update 2: save Tau\n",
        "\n",
        "        empty_weight = torch.ones(self.num_labels + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_weight)\n",
        "\n",
        "        self.matcher = Mask2FormerHungarianMatcher(\n",
        "            num_points=num_points,\n",
        "            cost_mask=mask_coefficient,\n",
        "            cost_dice=dice_coefficient,\n",
        "            cost_class=class_coefficient,\n",
        "        )\n",
        "\n",
        "    @torch.compiler.disable\n",
        "    def forward(\n",
        "        self,\n",
        "        masks_queries_logits: torch.Tensor,\n",
        "        targets: List[dict],\n",
        "        class_queries_logits: Optional[torch.Tensor] = None,\n",
        "    ):\n",
        "        mask_labels = [\n",
        "            target[\"masks\"].to(masks_queries_logits.dtype) for target in targets\n",
        "        ]\n",
        "        class_labels = [target[\"labels\"].long() for target in targets]\n",
        "\n",
        "        #! <--- Update 3: LOGIT NORMALIZATION\n",
        "        #! Normalize logits BEFORE they are used by the matcher or the loss.\n",
        "        #! This forces the network to optimize cosine similarity.\n",
        "        if class_queries_logits is not None:\n",
        "            #! Calculate L2 norm along the last dimension (classes)\n",
        "            norms = torch.norm(class_queries_logits, p=2, dim=-1, keepdim=True) + 1e-7\n",
        "            #! Normalize and scale by temperature (1/tau)\n",
        "            class_queries_logits = (class_queries_logits / norms) / self.logit_norm_tau\n",
        "\n",
        "        #! Now pass the normalized logits to both the matcher and the loss function\n",
        "        indices = self.matcher(\n",
        "            masks_queries_logits=masks_queries_logits,\n",
        "            mask_labels=mask_labels,\n",
        "            class_queries_logits=class_queries_logits, #! Now normalized logits are used here\n",
        "            class_labels=class_labels,\n",
        "        )\n",
        "\n",
        "        loss_masks = self.loss_masks(masks_queries_logits, mask_labels, indices)\n",
        "\n",
        "        #! Also use normalized logits here\n",
        "        loss_classes = self.loss_labels(class_queries_logits, class_labels, indices)\n",
        "\n",
        "        return {**loss_masks, **loss_classes}\n",
        "\n",
        "    def loss_masks(self, masks_queries_logits, mask_labels, indices):\n",
        "        loss_masks = super().loss_masks(masks_queries_logits, mask_labels, indices, 1)\n",
        "\n",
        "        num_masks = sum(len(tgt) for (_, tgt) in indices)\n",
        "        num_masks_tensor = torch.as_tensor(\n",
        "            num_masks, dtype=torch.float, device=masks_queries_logits.device\n",
        "        )\n",
        "\n",
        "        if dist.is_available() and dist.is_initialized():\n",
        "            dist.all_reduce(num_masks_tensor)\n",
        "            world_size = dist.get_world_size()\n",
        "        else:\n",
        "            world_size = 1\n",
        "\n",
        "        num_masks = torch.clamp(num_masks_tensor / world_size, min=1)\n",
        "\n",
        "        for key in loss_masks.keys():\n",
        "            loss_masks[key] = loss_masks[key] / num_masks\n",
        "\n",
        "        return loss_masks\n",
        "\n",
        "    def loss_total(self, losses_all_layers, log_fn) -> torch.Tensor:\n",
        "        loss_total = None\n",
        "        for loss_key, loss in losses_all_layers.items():\n",
        "            log_fn(f\"losses/train_{loss_key}\", loss, sync_dist=True)\n",
        "\n",
        "            if \"mask\" in loss_key:\n",
        "                weighted_loss = loss * self.mask_coefficient\n",
        "            elif \"dice\" in loss_key:\n",
        "                weighted_loss = loss * self.dice_coefficient\n",
        "            elif \"cross_entropy\" in loss_key:\n",
        "                weighted_loss = loss * self.class_coefficient\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown loss key: {loss_key}\")\n",
        "\n",
        "            if loss_total is None:\n",
        "                loss_total = weighted_loss\n",
        "            else:\n",
        "                loss_total = torch.add(loss_total, weighted_loss)\n",
        "\n",
        "        log_fn(\"losses/train_loss_total\", loss_total, sync_dist=True, prog_bar=True)\n",
        "\n",
        "        return loss_total  # type: ignore\n",
        "'''"
      ],
      "metadata": {
        "id": "ffpezw06RqSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Scriviamo il file nel percorso corretto\n",
        "with open(\"eomt/training/mask_classification_loss.py\", \"w\") as f:\n",
        "    f.write(loss_file_content)\n",
        "\n",
        "print(\">> File mask_classification_loss.py aggiornato con la LogitNorm!\")\n",
        "'''"
      ],
      "metadata": {
        "id": "STgsAafqR0-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nk0k8pNBuXVJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}