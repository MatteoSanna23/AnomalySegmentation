trainer:
  max_epochs: 107
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        every_n_epochs: 1
        save_last: true
  logger:
    class_path: lightning.pytorch.loggers.wandb.WandbLogger
    init_args:
      resume: allow
      project: "eomt"
      name: "cityscapes_semantic_eomt_base_640_lora"

model:
  class_path: training.mask_classification_semantic.MaskClassificationSemantic
  init_args:
    # 896 / 14 = 64 (Griglia 64x64). Combacia con il tuo checkpoint, NON cambiarlo se possibile.
    img_size: [896, 896] 
    num_classes: 19
    attn_mask_annealing_enabled: True
    attn_mask_annealing_start_steps: [3317, 8292, 13268]
    attn_mask_annealing_end_steps: [6634, 11609, 16585]
    network:
      class_path: models.eomt.EoMT
      init_args:
        num_q: 100
        num_blocks: 3
        num_classes: 19
        encoder:
          class_path: models.vit.ViT
          init_args:
            img_size: [896, 896]
            backbone_name: vit_base_patch14_reg4_dinov2
            patch_size: 16
        # LoRA Configuration Ottimizzata
        lora_config:
          class_path: models.lora_integration.LoRAConfig
          init_args:
            enabled: true
            rank: 8
            lora_alpha: 16
            lora_dropout: 0.1
            # MODIFICA CRUCIALE: Targettiamo i layer interni del ViT, non le teste.
            # qkv/proj = Attenzione, fc1/fc2 = MLP (Feed Forward)
            target_modules: ["qkv", "proj", "fc1", "fc2"] 
            
            # Nota importante: Assicurati che nel tuo codice Python, 
            # quando freeze_base_model è true, le "class_head" e "mask_head" 
            # vengano esplicitamente sbloccate (trainable=True), 
            # altrimenti il modello non imparerà a classificare.
            freeze_base_model: true

data:
  class_path: datasets.cityscapes_semantic.CityscapesSemantic
  init_args:
    path: /teamspace/studios/this_studio/datasets
    num_workers: 4
    # Occhio alla VRAM: 896x896 è pesante. Se vai OOM (Out of Memory), riduci batch_size a 1.
    batch_size: 2
    img_size: [896, 896]
    num_classes: 19